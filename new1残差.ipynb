{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kunry/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "#data处理\n",
    "class Corpus:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.load_cifar10('./cifar-10-batches-py')\n",
    "        self._split_train_valid(valid_rate=0.9)\n",
    "        self.n_train = self.train_images.shape[0]\n",
    "        self.n_valid = self.valid_images.shape[0]\n",
    "        self.n_test = self.test_images.shape[0]\n",
    "        \n",
    "    def _split_train_valid(self, valid_rate=0.9):\n",
    "        images, labels = self.train_images, self.train_labels \n",
    "        thresh = int(images.shape[0] * valid_rate)\n",
    "        self.train_images, self.train_labels = images[0:thresh,:,:,:], labels[0:thresh]\n",
    "        self.valid_images, self.valid_labels = images[thresh:,:,:,:], labels[thresh:]\n",
    "    \n",
    "    def load_cifar10(self, directory):\n",
    "        # 读取训练集\n",
    "        images, labels = [], []\n",
    "        for filename in ['%s/data_batch_%d' % (directory, j) for j in range(1, 6)]:\n",
    "            with open(filename, 'rb') as fo:\n",
    "                if 'Windows' in platform.platform():\n",
    "                    cifar10 = pickle.load(fo, encoding='bytes')\n",
    "                elif 'Linux' in platform.platform():\n",
    "                    cifar10 = pickle.load(fo)\n",
    "            for i in range(len(cifar10[b\"labels\"])):\n",
    "                image = numpy.reshape(cifar10[b\"data\"][i], (3, 32, 32))\n",
    "                image = numpy.transpose(image, (1, 2, 0))\n",
    "                image = image.astype(float)\n",
    "                images.append(image)\n",
    "            labels += cifar10[b\"labels\"]\n",
    "        images = numpy.array(images, dtype='float')\n",
    "        labels = numpy.array(labels, dtype='int')\n",
    "        self.train_images, self.train_labels = images, labels\n",
    "        # 读取测试集\n",
    "        images, labels = [], []\n",
    "        for filename in ['%s/test_batch' % (directory)]:\n",
    "            with open(filename, 'rb') as fo:\n",
    "                if 'Windows' in platform.platform():\n",
    "                    cifar10 = pickle.load(fo, encoding='bytes')\n",
    "                elif 'Linux' in platform.platform():\n",
    "                    cifar10 = pickle.load(fo)\n",
    "            for i in range(len(cifar10[b\"labels\"])):\n",
    "                image = numpy.reshape(cifar10[b\"data\"][i], (3, 32, 32))\n",
    "                image = numpy.transpose(image, (1, 2, 0))\n",
    "                image = image.astype(float)\n",
    "                images.append(image)\n",
    "            labels += cifar10[b\"labels\"]\n",
    "        images = numpy.array(images, dtype='float')\n",
    "        labels = numpy.array(labels, dtype='int')\n",
    "        self.test_images, self.test_labels = images, labels\n",
    "        \n",
    "    def data_augmentation(self, images, mode='train', flip=False, \n",
    "                          crop=False, crop_shape=(24,24,3), whiten=False, \n",
    "                          noise=False, noise_mean=0, noise_std=0.01):\n",
    "        # 图像切割\n",
    "        if crop:\n",
    "            if mode == 'train':\n",
    "                images = self._image_crop(images, shape=crop_shape)\n",
    "            elif mode == 'test':\n",
    "                images = self._image_crop_test(images, shape=crop_shape)\n",
    "        # 图像翻转\n",
    "        if flip:\n",
    "            images = self._image_flip(images)\n",
    "        # 图像白化\n",
    "        if whiten:\n",
    "            images = self._image_whitening(images)\n",
    "        # 图像噪声\n",
    "        if noise:\n",
    "            images = self._image_noise(images, mean=noise_mean, std=noise_std)\n",
    "            \n",
    "        return images\n",
    "    \n",
    "    def _image_crop(self, images, shape):\n",
    "        # 图像切割\n",
    "        new_images = []\n",
    "        for i in range(images.shape[0]):\n",
    "            old_image = images[i,:,:,:]\n",
    "            left = numpy.random.randint(old_image.shape[0] - shape[0] + 1)\n",
    "            top = numpy.random.randint(old_image.shape[1] - shape[1] + 1)\n",
    "            new_image = old_image[left: left+shape[0], top: top+shape[1], :]\n",
    "            new_images.append(new_image)\n",
    "        \n",
    "        return numpy.array(new_images)\n",
    "    \n",
    "    def _image_crop_test(self, images, shape):\n",
    "        # 图像切割\n",
    "        new_images = []\n",
    "        for i in range(images.shape[0]):\n",
    "            old_image = images[i,:,:,:]\n",
    "            left = int((old_image.shape[0] - shape[0]) / 2)\n",
    "            top = int((old_image.shape[1] - shape[1]) / 2)\n",
    "            new_image = old_image[left: left+shape[0], top: top+shape[1], :]\n",
    "            new_images.append(new_image)\n",
    "        \n",
    "        return numpy.array(new_images)\n",
    "    \n",
    "    def _image_flip(self, images):\n",
    "        # 图像翻转\n",
    "        for i in range(images.shape[0]):\n",
    "            old_image = images[i,:,:,:]\n",
    "            if numpy.random.random() < 0.5:\n",
    "                new_image = cv2.flip(old_image, 1)\n",
    "            else:\n",
    "                new_image = old_image\n",
    "            images[i,:,:,:] = new_image\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def _image_whitening(self, images):\n",
    "        # 图像白化\n",
    "        for i in range(images.shape[0]):\n",
    "            old_image = images[i,:,:,:]\n",
    "            new_image = (old_image - numpy.mean(old_image)) / numpy.std(old_image)\n",
    "            images[i,:,:,:] = new_image\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def _image_noise(self, images, mean=0, std=0.01):\n",
    "        # 图像噪声\n",
    "        for i in range(images.shape[0]):\n",
    "            old_image = images[i,:,:,:]\n",
    "            new_image = old_image\n",
    "            for i in range(image.shape[0]):\n",
    "                for j in range(image.shape[1]):\n",
    "                    for k in range(image.shape[2]):\n",
    "                        new_image[i, j, k] += random.gauss(mean, std)\n",
    "            images[i,:,:,:] = new_image\n",
    "        \n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#卷积基础框架\n",
    "class ConvLayer:\n",
    "    \n",
    "    def __init__(self, input_shape, n_size, n_filter, stride=1, activation='relu',\n",
    "                 batch_normal=False, weight_decay=None, name='conv'):\n",
    "        # params\n",
    "        self.input_shape = input_shape\n",
    "        self.n_filter = n_filter\n",
    "        self.activation = activation\n",
    "        self.stride = stride\n",
    "        self.batch_normal = batch_normal\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # 权重矩阵\n",
    "        self.weight = tf.Variable(\n",
    "            initial_value=tf.truncated_normal(\n",
    "                shape=[n_size, n_size, self.input_shape[3], self.n_filter],\n",
    "                mean=0.0, stddev=numpy.sqrt(\n",
    "                    2.0 / (self.input_shape[1] * self.input_shape[2] * self.input_shape[3]))),\n",
    "            name='W_%s' % (name))\n",
    "        \n",
    "        # weight decay技术\n",
    "        if self.weight_decay:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(self.weight), self.weight_decay)\n",
    "            tf.add_to_collection('losses', weight_decay)\n",
    "            \n",
    "        # 偏置向量\n",
    "        self.bias = tf.Variable(\n",
    "            initial_value=tf.constant(\n",
    "                0.0, shape=[self.n_filter]),\n",
    "            name='b_%s' % (name))\n",
    "        \n",
    "        # batch normalization 技术的参数\n",
    "        if self.batch_normal:\n",
    "            self.epsilon = 1e-5\n",
    "            self.gamma = tf.Variable(\n",
    "                initial_value=tf.constant(\n",
    "                    1.0, shape=[self.n_filter]),\n",
    "            name='gamma_%s' % (name))\n",
    "        \n",
    "    def get_output(self, input):\n",
    "        # calculate input_shape and output_shape\n",
    "        self.output_shape = [self.input_shape[0], int(self.input_shape[1]/self.stride),\n",
    "                             int(self.input_shape[2]/self.stride), self.n_filter]\n",
    "        \n",
    "        # hidden states\n",
    "        self.conv = tf.nn.conv2d(\n",
    "            input=input, filter=self.weight, \n",
    "            strides=[1, self.stride, self.stride, 1], padding='SAME')\n",
    "        \n",
    "        # batch normalization 技术\n",
    "        if self.batch_normal:\n",
    "            mean, variance = tf.nn.moments(self.conv, axes=[0, 1, 2], keep_dims=False)\n",
    "            self.hidden = tf.nn.batch_normalization(\n",
    "                self.conv, mean, variance, self.bias, self.gamma, self.epsilon)\n",
    "        else:\n",
    "            self.hidden = self.conv + self.bias\n",
    "            \n",
    "        # activation\n",
    "        if self.activation == 'relu':\n",
    "            self.output = tf.nn.relu(self.hidden)\n",
    "        elif self.activation == 'tanh':\n",
    "            self.output = tf.nn.tanh(self.hidden)\n",
    "        elif self.activation == 'none':\n",
    "            self.output = self.hidden\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#堆叠层\n",
    "class DenseLayer:\n",
    "    \n",
    "    def __init__(self, input_shape, hidden_dim, activation='relu', dropout=False, \n",
    "                 keep_prob=None, batch_normal=False, weight_decay=None, name='dense'):\n",
    "        # params\n",
    "        self.input_shape = input_shape\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.batch_normal = batch_normal\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # 权重矩阵\n",
    "        self.weight = tf.Variable(\n",
    "            initial_value=tf.random_normal(\n",
    "                shape=[self.input_shape[1], self.hidden_dim],\n",
    "                mean=0.0, stddev=numpy.sqrt(2.0 / self.input_shape[1])),\n",
    "            name='W_%s' % (name))\n",
    "        \n",
    "        # weight decay技术\n",
    "        if weight_decay:\n",
    "            weight_decay = tf.multiply(tf.nn.l2_loss(self.weight), self.weight_decay)\n",
    "            tf.add_to_collection('losses', weight_decay)\n",
    "            \n",
    "        # 偏置向量\n",
    "        self.bias = tf.Variable(\n",
    "            initial_value=tf.constant(\n",
    "                0.0, shape=[self.hidden_dim]),\n",
    "            name='b_%s' % (name))\n",
    "        \n",
    "        # batch normalization 技术的参数\n",
    "        if self.batch_normal:\n",
    "            self.epsilon = 1e-5\n",
    "            self.gamma = tf.Variable(\n",
    "                initial_value=tf.constant(\n",
    "                    1.0, shape=[self.hidden_dim]),\n",
    "            name='gamma_%s' % (name))\n",
    "        # dropout 技术\n",
    "        if self.dropout:\n",
    "            self.keep_prob = keep_prob\n",
    "        \n",
    "    def get_output(self, input):\n",
    "        # calculate input_shape and output_shape\n",
    "        self.output_shape = [self.input_shape[0], self.hidden_dim]\n",
    "        # hidden states\n",
    "        intermediate = tf.matmul(input, self.weight)\n",
    "        \n",
    "        # batch normalization 技术\n",
    "        if self.batch_normal:\n",
    "            mean, variance = tf.nn.moments(intermediate, axes=[0])\n",
    "            self.hidden = tf.nn.batch_normalization(\n",
    "                intermediate, mean, variance, self.bias, self.gamma, self.epsilon)\n",
    "        else:\n",
    "            self.hidden = intermediate + self.bias\n",
    "            \n",
    "        # dropout 技术\n",
    "        if self.dropout:\n",
    "            self.hidden = tf.nn.dropout(self.hidden, keep_prob=self.keep_prob)\n",
    "            \n",
    "        # activation\n",
    "        if self.activation == 'relu':\n",
    "            self.output = tf.nn.relu(self.hidden)\n",
    "        elif self.activation == 'tanh':\n",
    "            self.output = tf.nn.tanh(self.hidden)\n",
    "        elif self.activation == 'softmax':\n",
    "            self.output = tf.nn.softmax(self.hidden)\n",
    "        elif self.activation == 'none':\n",
    "            self.output = self.hidden\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#残差网络\n",
    "class ConvNet():\n",
    "    \n",
    "    def __init__(self, n_channel=3, n_classes=10, image_size=24, n_layers=20):\n",
    "        # 设置超参数\n",
    "        self.n_channel = n_channel\n",
    "        self.n_classes = n_classes\n",
    "        self.image_size = image_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 输入变量\n",
    "        self.images = tf.placeholder(\n",
    "            dtype=tf.float32, shape=[None, self.image_size, self.image_size, self.n_channel], \n",
    "            name='images')\n",
    "        self.labels = tf.placeholder(\n",
    "            dtype=tf.int64, shape=[None], name='labels')\n",
    "        self.keep_prob = tf.placeholder(\n",
    "            dtype=tf.float32, name='keep_prob')\n",
    "        self.global_step = tf.Variable(\n",
    "            0, dtype=tf.int32, name='global_step')\n",
    "        \n",
    "        # 网络输出\n",
    "        self.logits = self.inference(self.images)\n",
    "        \n",
    "        # 目标函数\n",
    "        self.objective = tf.reduce_sum(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self.logits, labels=self.labels))\n",
    "        tf.add_to_collection('losses', self.objective)\n",
    "        self.avg_loss = tf.add_n(tf.get_collection('losses'))\n",
    "        # 优化器\n",
    "        lr = tf.cond(tf.less(self.global_step, 50000), \n",
    "                     lambda: tf.constant(0.01),\n",
    "                     lambda: tf.cond(tf.less(self.global_step, 100000),\n",
    "                                     lambda: tf.constant(0.005),\n",
    "                                     lambda: tf.cond(tf.less(self.global_step, 150000),\n",
    "                                                     lambda: tf.constant(0.0025),\n",
    "                                                     lambda: tf.constant(0.001))))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(\n",
    "            self.avg_loss, global_step=self.global_step)\n",
    "        \n",
    "        # 观察值\n",
    "        correct_prediction = tf.equal(self.labels, tf.argmax(self.logits, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "        \n",
    "    def inference(self, images):\n",
    "        n_layers = int((self.n_layers - 2) / 6)\n",
    "        # 网络结构\n",
    "        conv_layer0_list = []\n",
    "        conv_layer0_list.append(\n",
    "            ConvLayer(\n",
    "                input_shape=(None, self.image_size, self.image_size, self.n_channel), \n",
    "                n_size=3, n_filter=64, stride=1, activation='relu', \n",
    "                batch_normal=True, weight_decay=1e-4, name='conv0'))\n",
    "        \n",
    "        conv_layer1_list = []\n",
    "        for i in range(1, n_layers+1):\n",
    "            conv_layer1_list.append(\n",
    "                ConvLayer(\n",
    "                    input_shape=(None, self.image_size, self.image_size, 64), \n",
    "                    n_size=3, n_filter=64, stride=1, activation='relu', \n",
    "                    batch_normal=True, weight_decay=1e-4, name='conv1_%d' % (2*i-1)))\n",
    "            conv_layer1_list.append(\n",
    "                ConvLayer(\n",
    "                    input_shape=(None, self.image_size, self.image_size, 64), \n",
    "                    n_size=3, n_filter=64, stride=1, activation='none', \n",
    "                    batch_normal=True, weight_decay=1e-4, name='conv1_%d' % (2*i)))\n",
    "        \n",
    "        conv_layer2_list = []\n",
    "        conv_layer2_list.append(\n",
    "            ConvLayer(\n",
    "                input_shape=(None, self.image_size, self.image_size, 64), \n",
    "                n_size=3, n_filter=128, stride=2, activation='relu', \n",
    "                batch_normal=True, weight_decay=1e-4, name='conv2_1'))\n",
    "        conv_layer2_list.append(\n",
    "            ConvLayer(\n",
    "                input_shape=(None, int(self.image_size)/2, int(self.image_size)/2, 128), \n",
    "                n_size=3, n_filter=128, stride=1, activation='none', \n",
    "                batch_normal=True, weight_decay=1e-4, name='conv2_2'))\n",
    "        for i in range(2, n_layers+1):\n",
    "            conv_layer2_list.append(\n",
    "                ConvLayer(\n",
    "                    input_shape=(None, int(self.image_size/2), int(self.image_size/2), 128), \n",
    "                    n_size=3, n_filter=128, stride=1, activation='relu', \n",
    "                    batch_normal=True, weight_decay=1e-4, name='conv2_%d' % (2*i-1)))\n",
    "            conv_layer2_list.append(\n",
    "                ConvLayer(\n",
    "                    input_shape=(None, int(self.image_size/2), int(self.image_size/2), 128), \n",
    "                    n_size=3, n_filter=128, stride=1, activation='none', \n",
    "                    batch_normal=True, weight_decay=1e-4, name='conv2_%d' % (2*i)))\n",
    "        \n",
    "        conv_layer3_list = []\n",
    "        conv_layer3_list.append(\n",
    "            ConvLayer(\n",
    "                input_shape=(None, int(self.image_size/2), int(self.image_size/2), 128), \n",
    "                n_size=3, n_filter=256, stride=2, activation='relu', \n",
    "                batch_normal=True, weight_decay=1e-4, name='conv3_1'))\n",
    "        conv_layer3_list.append(\n",
    "            ConvLayer(\n",
    "                input_shape=(None, int(self.image_size/4), int(self.image_size/4), 256), \n",
    "                n_size=3, n_filter=256, stride=1, activation='relu', \n",
    "                batch_normal=True, weight_decay=1e-4, name='conv3_2'))\n",
    "        for i in range(2, n_layers+1):\n",
    "            conv_layer3_list.append(\n",
    "                ConvLayer(\n",
    "                    input_shape=(None, int(self.image_size/4), int(self.image_size/4), 256), \n",
    "                    n_size=3, n_filter=256, stride=1, activation='relu', \n",
    "                    batch_normal=True, weight_decay=1e-4, name='conv3_%d' % (2*i-1)))\n",
    "            conv_layer3_list.append(\n",
    "                ConvLayer(\n",
    "                    input_shape=(None, int(self.image_size/4), int(self.image_size/4), 256), \n",
    "                    n_size=3, n_filter=256, stride=1, activation='none', \n",
    "                    batch_normal=True, weight_decay=1e-4, name='conv3_%d' % (2*i)))\n",
    "        \n",
    "        dense_layer1 = DenseLayer(\n",
    "            input_shape=(None, 256),\n",
    "            hidden_dim=self.n_classes,\n",
    "            activation='none', dropout=False, keep_prob=None, \n",
    "            batch_normal=False, weight_decay=1e-4, name='dense1')\n",
    "        \n",
    "        # 数据流\n",
    "        hidden_conv = conv_layer0_list[0].get_output(input=images)\n",
    "        \n",
    "        for i in range(0, n_layers):\n",
    "            hidden_conv1 = conv_layer1_list[2*i].get_output(input=hidden_conv)\n",
    "            hidden_conv2 = conv_layer1_list[2*i+1].get_output(input=hidden_conv1)\n",
    "            hidden_conv = tf.nn.relu(hidden_conv + hidden_conv2)\n",
    "            \n",
    "        hidden_conv1 = conv_layer2_list[0].get_output(input=hidden_conv)\n",
    "        hidden_conv2 = conv_layer2_list[1].get_output(input=hidden_conv1)\n",
    "        hidden_pool = tf.nn.max_pool(\n",
    "            hidden_conv, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        hidden_pad = tf.pad(hidden_pool, [[0,0], [0,0], [0,0], [32,32]])\n",
    "        hidden_conv = tf.nn.relu(hidden_pad + hidden_conv2)\n",
    "        for i in range(1, n_layers):\n",
    "            hidden_conv1 = conv_layer2_list[2*i].get_output(input=hidden_conv)\n",
    "            hidden_conv2 = conv_layer2_list[2*i+1].get_output(input=hidden_conv1)\n",
    "            hidden_conv = tf.nn.relu(hidden_conv + hidden_conv2)\n",
    "        \n",
    "        hidden_conv1 = conv_layer3_list[0].get_output(input=hidden_conv)\n",
    "        hidden_conv2 = conv_layer3_list[1].get_output(input=hidden_conv1)\n",
    "        hidden_pool = tf.nn.max_pool(\n",
    "            hidden_conv, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        hidden_pad = tf.pad(hidden_pool, [[0,0], [0,0], [0,0], [64,64]])\n",
    "        hidden_conv = tf.nn.relu(hidden_pad + hidden_conv2)\n",
    "        for i in range(1, n_layers):\n",
    "            hidden_conv1 = conv_layer3_list[2*i].get_output(input=hidden_conv)\n",
    "            hidden_conv2 = conv_layer3_list[2*i+1].get_output(input=hidden_conv1)\n",
    "            hidden_conv = tf.nn.relu(hidden_conv + hidden_conv2)\n",
    "            \n",
    "        # global average pooling\n",
    "        input_dense1 = tf.reduce_mean(hidden_conv, reduction_indices=[1, 2])\n",
    "        logits = dense_layer1.get_output(input=input_dense1)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "    def train(self, dataloader, backup_path, n_epoch=5, batch_size=128):\n",
    "        # 构建会话\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "        # 模型保存器\n",
    "        self.saver = tf.train.Saver(\n",
    "            var_list=tf.global_variables(), write_version=tf.train.SaverDef.V2, \n",
    "            max_to_keep=5)\n",
    "        # 模型初始化\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # 验证集数据增强\n",
    "        valid_images = dataloader.data_augmentation(dataloader.valid_images, mode='test',\n",
    "            flip=False, crop=True, crop_shape=(24,24,3), whiten=True, noise=False)\n",
    "        valid_labels = dataloader.valid_labels\n",
    "        # 模型训练\n",
    "        for epoch in range(0, n_epoch+1):\n",
    "            # 训练集数据增强\n",
    "            train_images = dataloader.data_augmentation(dataloader.train_images, mode='train',\n",
    "                flip=True, crop=True, crop_shape=(24,24,3), whiten=True, noise=False)\n",
    "            train_labels = dataloader.train_labels\n",
    "            \n",
    "            # 开始本轮的训练，并计算目标函数值\n",
    "            train_loss = 0.0\n",
    "            for i in range(0, dataloader.n_train, batch_size):\n",
    "                batch_images = train_images[i: i+batch_size]\n",
    "                batch_labels = train_labels[i: i+batch_size]\n",
    "                [_, avg_loss, iteration] = self.sess.run(\n",
    "                    fetches=[self.optimizer, self.avg_loss, self.global_step], \n",
    "                    feed_dict={self.images: batch_images, \n",
    "                               self.labels: batch_labels, \n",
    "                               self.keep_prob: 0.5})\n",
    "                \n",
    "                train_loss += avg_loss * batch_images.shape[0]\n",
    "            train_loss = 1.0 * train_loss / dataloader.n_train\n",
    "            \n",
    "            # 在训练之后，获得本轮的验证集损失值和准确率\n",
    "            valid_accuracy, valid_loss = 0.0, 0.0\n",
    "            for i in range(0, dataloader.n_valid, batch_size):\n",
    "                batch_images = valid_images[i: i+batch_size]\n",
    "                batch_labels = valid_labels[i: i+batch_size]\n",
    "                [avg_accuracy, avg_loss] = self.sess.run(\n",
    "                    fetches=[self.accuracy, self.avg_loss], \n",
    "                    feed_dict={self.images: batch_images, \n",
    "                               self.labels: batch_labels, \n",
    "                               self.keep_prob: 1.0})\n",
    "                valid_accuracy += avg_accuracy * batch_images.shape[0]\n",
    "                valid_loss += avg_loss * batch_images.shape[0]\n",
    "            valid_accuracy = 1.0 * valid_accuracy / dataloader.n_valid\n",
    "            valid_loss = 1.0 * valid_loss / dataloader.n_valid\n",
    "            \n",
    "            print('epoch{%d}, iter[%d], train loss: %.6f, '\n",
    "                  'valid precision: %.6f, valid loss: %.6f' % (\n",
    "                epoch, iteration, train_loss, valid_accuracy, valid_loss))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # 保存模型\n",
    "            if epoch <= 1000 and epoch % 100 == 0 or \\\n",
    "                epoch <= 10000 and epoch % 1000 == 0:\n",
    "                saver_path = self.saver.save(\n",
    "                    self.sess, os.path.join(backup_path, 'model_%d.ckpt' % (epoch)))\n",
    "                \n",
    "        self.sess.close()\n",
    "                \n",
    "    def test(self, dataloader, backup_path, epoch, batch_size=128):\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "        # 读取模型\n",
    "        self.saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "        model_path = os.path.join(backup_path, 'model_%d.ckpt' % (epoch))\n",
    "        assert(os.path.exists(model_path+'.index'))\n",
    "        self.saver.restore(self.sess, model_path)\n",
    "        print('read model from %s' % (model_path))\n",
    "        # 在测试集上计算准确率\n",
    "        accuracy_list = []\n",
    "        test_images = dataloader.data_augmentation(dataloader.test_images,\n",
    "            flip=False, crop=True, crop_shape=(24,24,3), whiten=True, noise=False)\n",
    "        test_labels = dataloader.test_labels\n",
    "        for i in range(0, dataloader.n_test, batch_size):\n",
    "            batch_images = test_images[i: i+batch_size]\n",
    "            batch_labels = test_labels[i: i+batch_size]\n",
    "            [avg_accuracy] = self.sess.run(\n",
    "                fetches=[self.accuracy], \n",
    "                feed_dict={self.images:batch_images, \n",
    "                           self.labels:batch_labels,\n",
    "                           self.keep_prob:1.0})\n",
    "            accuracy_list.append(avg_accuracy)\n",
    "        print('test precision: %.4f' % (numpy.mean(accuracy_list)))\n",
    "        self.sess.close()\n",
    "            \n",
    "    def debug(self):\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        [temp] = sess.run(\n",
    "            fetches=[self.logits],\n",
    "            feed_dict={self.images: numpy.random.random(size=[128, 24, 24, 3]),\n",
    "                       self.labels: numpy.random.randint(low=0, high=9, size=[128,]),\n",
    "                       self.keep_prob: 1.0})\n",
    "        print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kunry/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "class_num = 10\n",
    "image_size = 32\n",
    "img_channels = 3\n",
    "iterations = 250  #迭代周期，总数据/batch_size，200*250=50000\n",
    "batch_size = 200\n",
    "total_epoch = 350\n",
    "log_save_path = './simple_logs'\n",
    "model_save_path = './model/'\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data():  #下载\n",
    "    dirname = 'cifar-10-batches-py'\n",
    "    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    fname = './CAFIR-10_data/cifar-10-python.tar.gz'\n",
    "    fpath = './' + dirname\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath) or os.path.isfile(fname):\n",
    "        download = False\n",
    "        print(\"DataSet already exist!\")\n",
    "    else:\n",
    "        download = True\n",
    "    if download:\n",
    "        print('Downloading data from', origin)\n",
    "        import urllib.request\n",
    "        import tarfile\n",
    "\n",
    "        def reporthook(count, block_size, total_size):\n",
    "            global start_time\n",
    "            if count == 0:\n",
    "                start_time = time.time()\n",
    "                return\n",
    "            duration = time.time() - start_time\n",
    "            progress_size = int(count * block_size)\n",
    "            speed = int(progress_size / (1024 * duration))\n",
    "            percent = min(int(count*block_size*100/total_size),100)\n",
    "            sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
    "                            (percent, progress_size / (1024 * 1024), speed, duration))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        urllib.request.urlretrieve(origin, fname, reporthook)\n",
    "        print('Download finished. Start extract!', origin)\n",
    "        if fname.endswith(\"tar.gz\"):\n",
    "            tar = tarfile.open(fname, \"r:gz\")\n",
    "            tar.extractall()\n",
    "            tar.close()\n",
    "        elif fname.endswith(\"tar\"):\n",
    "            tar = tarfile.open(fname, \"r:\")\n",
    "            tar.extractall()\n",
    "            tar.close()\n",
    "\n",
    "#源数据解压\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "#源数据处理\n",
    "def load_data_one(file):\n",
    "    batch = unpickle(file)\n",
    "    data = batch[b'data']\n",
    "    labels = batch[b'labels']\n",
    "    print(\"Loading %s : %d.\" % (file, len(data)))\n",
    "    return data, labels\n",
    "\n",
    "#源数据处理加载\n",
    "def load_data(files, data_dir, label_count):\n",
    "    global image_size, img_channels\n",
    "    data, labels = load_data_one(data_dir + '/' + files[0])\n",
    "    for f in files[1:]:\n",
    "        data_n, labels_n = load_data_one(data_dir + '/' + f)\n",
    "        data = np.append(data, data_n, axis=0)\n",
    "        labels = np.append(labels, labels_n, axis=0)\n",
    "    labels = np.array([[float(i == label) for i in range(label_count)] for label in labels])\n",
    "    data = data.reshape([-1, img_channels, image_size, image_size])\n",
    "    data = data.transpose([0, 2, 3, 1])\n",
    "    return data, labels\n",
    "\n",
    "#源数据整合过程\n",
    "def prepare_data():\n",
    "    print(\"======Loading data======\")\n",
    "    download_data()\n",
    "    data_dir = './cifar-10-batches-py'\n",
    "    image_dim = image_size * image_size * img_channels\n",
    "    meta = unpickle(data_dir + '/batches.meta')\n",
    "\n",
    "    label_names = meta[b'label_names']\n",
    "    label_count = len(label_names)\n",
    "    train_files = ['data_batch_%d' % d for d in range(1, 6)]\n",
    "    train_data, train_labels = load_data(train_files, data_dir, label_count)\n",
    "    test_data, test_labels = load_data(['test_batch'], data_dir, label_count)\n",
    "\n",
    "    print(\"Train data:\", np.shape(train_data), np.shape(train_labels))\n",
    "    print(\"Test data :\", np.shape(test_data), np.shape(test_labels))\n",
    "    print(\"======Load finished======\")\n",
    "\n",
    "    print(\"======Shuffling data======\")\n",
    "    indices = np.random.permutation(len(train_data))\n",
    "    train_data = train_data[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "    print(\"======Prepare Finished======\")\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "#图片随机裁剪\n",
    "def _random_crop(batch, crop_shape, padding=None):\n",
    "    oshape = np.shape(batch[0])\n",
    "\n",
    "    if padding:\n",
    "        oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\n",
    "    new_batch = []\n",
    "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
    "    for i in range(len(batch)):\n",
    "        new_batch.append(batch[i])\n",
    "        if padding:\n",
    "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
    "                                      mode='constant', constant_values=0)\n",
    "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
    "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
    "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
    "                                    nw:nw + crop_shape[1]]\n",
    "    return new_batch\n",
    "\n",
    "\n",
    "#图片随机左右翻转\n",
    "def _random_flip_leftright(batch):\n",
    "        for i in range(len(batch)):\n",
    "            if bool(random.getrandbits(1)):\n",
    "                batch[i] = np.fliplr(batch[i])\n",
    "        return batch\n",
    "\n",
    "    \n",
    "#数据预处理-颜色处理并特征标准化，即图片白化。0，1，2为3原色通道。高级写法\n",
    "#http://ufldl.stanford.edu/wiki/index.php/Data_Preprocessing\n",
    "#https://www.cnblogs.com/luxiao/p/5534393.html\n",
    "def data_preprocessing(x_train,x_test):   \n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])\n",
    "    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])\n",
    "    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])\n",
    "\n",
    "    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])\n",
    "    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])\n",
    "    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "#数据增强\n",
    "def data_augmentation(batch): \n",
    "    batch = _random_flip_leftright(batch) #图片随机翻转\n",
    "    batch = _random_crop(batch, [32, 32], 4) #随机切割\n",
    "    return batch\n",
    "#############################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_rate_schedule(epoch_num): #按区间控制learning rate，可能这种效果比一次一次降低学习率要好，需要文献支撑\n",
    "    if epoch_num < 160:\n",
    "        return 0.1\n",
    "    elif epoch_num < 240:\n",
    "        return 0.01\n",
    "    elif epoch_num < 320:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "    \n",
    "# 测试集数据取值测试\n",
    "def run_testing(sess, ep):\n",
    "    acc = 0.0\n",
    "    loss = 0.0\n",
    "    pre_index = 0\n",
    "    add = 1000\n",
    "    for it in range(10):\n",
    "        batch_x = test_x[pre_index:pre_index + add]  # 索引，一个batch一次取1000行，也就是1000张测试集照片，循环10次，回到0——1000行取\n",
    "        batch_y = test_y[pre_index:pre_index + add]\n",
    "        pre_index = pre_index + add  # 累加实现索引区间递增\n",
    "\n",
    "        loss_, acc_ = sess.run([cross_entropy, accuracy],\n",
    "                               feed_dict={x: batch_x, y_: batch_y})\n",
    "\n",
    "        loss += loss_ / 10.0  # 结果的权重只占测试集跑出来的10%，累加，因为跑一次数据只有总数的10%，这样的结果应该合理。\n",
    "        acc += acc_ / 10.0\n",
    "\n",
    "\n",
    "        # 为tensorboard结构图用的\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag=\"Loss_1\", simple_value=loss),\n",
    "                                tf.Summary.Value(tag=\"Acc_1\", simple_value=acc)])\n",
    "\n",
    "    return acc, loss, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#block\n",
    "class Block(collections.namedtuple('Block', ['scope', 'unit_fn', 'args'])):\n",
    "    \"\"\"A named tuple describing a ResNet block.\"\"\"\n",
    "#采样    \n",
    "def subsample(inputs, factor, scope=None):\n",
    "    if factor == 1:\n",
    "        return inputs\n",
    "    else:\n",
    "        return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n",
    "\n",
    "def conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n",
    "    if stride == 1:\n",
    "        return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate, padding='SAME', scope=scope)\n",
    "    else:\n",
    "        kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n",
    "        pad_total = kernel_size_effective - 1\n",
    "        pad_beg = pad_total // 2\n",
    "        pad_end = pad_total - pad_beg\n",
    "        inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n",
    "        return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride, rate=rate, padding='VALID', scope=scope)\n",
    "#堆叠\n",
    "@slim.add_arg_scope\n",
    "def stack_blocks_dense(net, blocks, output_stride=None, store_non_strided_activations=False, outputs_collections=None):\n",
    "    current_stride = 1\n",
    "\n",
    "  # The atrous convolution rate parameter.\n",
    "    rate = 1\n",
    "    for block in blocks:\n",
    "        with tf.variable_scope(block.scope, 'block', [net]) as sc:\n",
    "            block_stride = 1\n",
    "            for i, unit in enumerate(block.args):\n",
    "                if store_non_strided_activations and i == len(block.args) - 1:\n",
    "                    block_stride = unit.get('stride', 1)\n",
    "                    unit = dict(unit, stride=1)\n",
    "                \n",
    "                with tf.variable_scope('unit_%d' % (i + 1), values=[net]):\n",
    "                    if output_stride is not None and current_stride == output_stride:\n",
    "                        net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n",
    "                        rate *= unit.get('stride', 1)\n",
    "                    else:\n",
    "                        net = block.unit_fn(net, rate=1, **unit)\n",
    "                        current_stride *= unit.get('stride', 1)\n",
    "                        if output_stride is not None and current_stride > output_stride:\n",
    "                            raise ValueError('The target output_stride cannot be reached.')\n",
    "\n",
    "        # Collect activations at the block's end before performing subsampling.\n",
    "            net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n",
    "        # Subsampling of the block's output activations.\n",
    "            if output_stride is not None and current_stride == output_stride:\n",
    "                rate *= block_stride\n",
    "            else:\n",
    "                net = subsample(net, block_stride)\n",
    "                current_stride *= block_stride\n",
    "        if output_stride is not None and current_stride > output_stride:\n",
    "            raise ValueError('The target output_stride cannot be reached.')\n",
    "\n",
    "    if output_stride is not None and current_stride != output_stride:\n",
    "        raise ValueError('The target output_stride cannot be reached.')\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#超参\n",
    "def resnet_arg_scope(weight_decay=0.0001,\n",
    "                     batch_norm_decay=0.997,\n",
    "                     batch_norm_epsilon=1e-5,\n",
    "                     batch_norm_scale=True,\n",
    "                     activation_fn=tf.nn.relu,\n",
    "                     use_batch_norm=True):\n",
    "    batch_norm_params = {\n",
    "      'decay': batch_norm_decay,\n",
    "      'epsilon': batch_norm_epsilon,\n",
    "      'scale': batch_norm_scale,\n",
    "      'updates_collections': tf.GraphKeys.UPDATE_OPS,\n",
    "      'fused': None,  # Use fused batch norm if possible.\n",
    "    }\n",
    "    \n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d],\n",
    "        weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "        weights_initializer=slim.variance_scaling_initializer(),\n",
    "        activation_fn=activation_fn,\n",
    "        normalizer_fn=slim.batch_norm if use_batch_norm else None,\n",
    "        normalizer_params=batch_norm_params):\n",
    "        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "            with slim.arg_scope([slim.max_pool2d], padding='SAME') as arg_sc:\n",
    "                return arg_sc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shortcut\n",
    "@slim.add_arg_scope\n",
    "def bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n",
    "               outputs_collections=None, scope=None):\n",
    "    with tf.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:\n",
    "        depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
    "        preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope='preact')\n",
    "        if depth == depth_in:\n",
    "            shortcut = subsample(inputs, stride, 'shortcut')\n",
    "        else:\n",
    "            shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride, normalizer_fn=None,\n",
    "                                   activation_fn=None, scope='shortcut')\n",
    "\n",
    "        residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n",
    "                               scope='conv1')\n",
    "        residual = conv2d_same(residual, depth_bottleneck, 3, stride,\n",
    "                               rate=rate, scope='conv2')\n",
    "        residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n",
    "                               normalizer_fn=None, activation_fn=None,\n",
    "                               scope='conv3')\n",
    "\n",
    "        output = shortcut + residual\n",
    "\n",
    "        return slim.utils.collect_named_outputs(outputs_collections, sc.name, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#resNet主结构\n",
    "def resnet_v2(inputs,\n",
    "              blocks,\n",
    "              num_classes=None,\n",
    "              is_training=True,\n",
    "              global_pool=True,\n",
    "              output_stride=None,\n",
    "              include_root_block=True,\n",
    "              spatial_squeeze=True,\n",
    "              reuse=None,\n",
    "              scope=None):\n",
    "    with tf.variable_scope(scope, 'resnet_v2', [inputs], reuse=reuse) as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        with slim.arg_scope([slim.conv2d, bottleneck,\n",
    "                             stack_blocks_dense],\n",
    "                            outputs_collections=end_points_collection):\n",
    "            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
    "                net = inputs\n",
    "                if include_root_block:\n",
    "                    if output_stride is not None:\n",
    "                        if output_stride % 4 != 0:\n",
    "                            raise ValueError('The output_stride needs to be a multiple of 4.')\n",
    "                        output_stride /= 4\n",
    "                    with slim.arg_scope([slim.conv2d],\n",
    "                                        activation_fn=None, normalizer_fn=None):\n",
    "                        #net = conv2d_same(net, 64, 7, stride=2, scope='conv1')\n",
    "                        net = conv2d_same(net, 64, 7, stride=2, scope='conv1')    #开始第一层\n",
    "                    net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1')\n",
    "                net = stack_blocks_dense(net, blocks, output_stride)\n",
    "                net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope='postnorm')\n",
    "                # Convert end_points_collection into a dictionary of end_points.\n",
    "                end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "                \n",
    "                if global_pool:\n",
    "                # Global average pooling.\n",
    "                    net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)\n",
    "                    end_points['global_pool'] = net\n",
    "                if num_classes:\n",
    "                    net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
    "                                      normalizer_fn=None, scope='logits')\n",
    "                    end_points[sc.name + '/logits'] = net\n",
    "                    if spatial_squeeze:\n",
    "                        net = tf.squeeze(net, [1, 2], name='SpatialSqueeze')\n",
    "                        end_points[sc.name + '/spatial_squeeze'] = net\n",
    "                    end_points['predictions'] = slim.softmax(net, scope='predictions')\n",
    "                return net, end_points\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet_v2_block(scope, base_depth, num_units, stride):\n",
    "    return Block(scope, bottleneck, [{\n",
    "        'depth': base_depth * 4,\n",
    "        'depth_bottleneck': base_depth,\n",
    "        'stride': 1\n",
    "    }] * (num_units - 1) + [{\n",
    "        'depth': base_depth * 4,\n",
    "        'depth_bottleneck': base_depth,\n",
    "        'stride': stride\n",
    "    }])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#50层\n",
    "def resnet_v2_50(inputs,\n",
    "                 num_classes=None,\n",
    "                 is_training=True,\n",
    "                 global_pool=True,\n",
    "                 output_stride=None,\n",
    "                 spatial_squeeze=True,\n",
    "                 reuse=None,\n",
    "                 scope='resnet_v2_50'):\n",
    "    blocks = [\n",
    "    resnet_v2_block('block1', base_depth=64, num_units=3, stride=2),\n",
    "    resnet_v2_block('block2', base_depth=128, num_units=4, stride=2),\n",
    "    resnet_v2_block('block3', base_depth=256, num_units=6, stride=2),\n",
    "    resnet_v2_block('block4', base_depth=512, num_units=3, stride=1),\n",
    "  ]\n",
    "    return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n",
    "                   global_pool=global_pool, output_stride=output_stride,\n",
    "                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n",
    "                   reuse=reuse, scope=scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Loading data======\n",
      "DataSet already exist!\n",
      "Loading ./cifar-10-batches-py/data_batch_1 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_2 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_3 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_4 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_5 : 10000.\n",
      "Loading ./cifar-10-batches-py/test_batch : 10000.\n",
      "Train data: (50000, 32, 32, 3) (50000, 10)\n",
      "Test data : (10000, 32, 32, 3) (10000, 10)\n",
      "======Load finished======\n",
      "======Shuffling data======\n",
      "======Prepare Finished======\n",
      "\n",
      " epoch 1/250:\n",
      "iteration: 2/2, cost_time: 107s, train_loss: 2.2940, train_acc: 0.1150, test_loss: 2.3033, test_acc: 0.1058\n",
      "\n",
      " epoch 2/250:\n",
      "iteration: 2/2, cost_time: 102s, train_loss: 2.2777, train_acc: 0.1700, test_loss: 2.2981, test_acc: 0.1196\n",
      "\n",
      " epoch 3/250:\n",
      "iteration: 1/2, train_loss: 2.2807, train_acc: 0.1650\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a0d11d491404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m                                                       tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                     \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 传到测试集函数区域做计算取值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mtest_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-425bf01f7d46>\u001b[0m in \u001b[0;36mrun_testing\u001b[0;34m(sess, ep)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         loss_, acc_ = sess.run([cross_entropy, accuracy],\n\u001b[0;32m---> 23\u001b[0;31m                                feed_dict={x: batch_x, y_: batch_y})\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10.0\u001b[0m  \u001b[0;31m# 结果的权重只占测试集跑出来的10%，累加，因为跑一次数据只有总数的10%，这样的结果应该合理。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 开始训练\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_x, train_y, test_x, test_y = prepare_data()  # 调用上面拆包出来的原数据\n",
    "    train_x, test_x = data_preprocessing(train_x, test_x)    #白化预处理数据\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, image_size, image_size, 3])\n",
    "    y_ = tf.placeholder(tf.float32, [None, class_num])\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \n",
    "#     arg_scope=resnet_arg_scope()\n",
    "#     with slim.arg_scope(arg_scope):\n",
    "#         my_net,my_end_points=resnet_v2_50(x, num_classes = 10, is_training=True)\n",
    "#     output = my_end_points['predictions'] \n",
    "        \n",
    "        \n",
    "    with slim.arg_scope(resnet_arg_scope()):\n",
    "        net, end_points = resnet_v2_50(x, 10, is_training=True, scope='resnet')\n",
    "    output = end_points['predictions']\n",
    "            \n",
    "\n",
    "    # loss函数\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output, name=\"Loss\"))  # 损失函数交叉熵\n",
    "    tf.summary.scalar(\"Loss\", cross_entropy)  # tensorboard记录值\n",
    "\n",
    "\n",
    "    # 优化器\n",
    "    #train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    train_step = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32, name=\"Acc\"))  # 正确率\n",
    "    tf.summary.scalar(\"Acc\", accuracy)  # tensorboard记录值\n",
    "\n",
    "    # initial an saver to save model  开始启动\n",
    "    saver = tf.train.Saver()\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(log_save_path, sess.graph)  # tensorboard流程图储存路径\n",
    "        # 写入tensorboard\n",
    "        train_writer = tf.summary.FileWriter(\"logs/Atrain_data_augmentation\", sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(\"logs/Atest_data_augmentation\", sess.graph)\n",
    "\n",
    "        # epoch = 164\n",
    "        # make sure [bath_size * iteration = data_set_number]\n",
    "\n",
    "        for ep in range(1, total_epoch + 1):\n",
    "            lr = learning_rate_schedule(ep)\n",
    "            pre_index = 0\n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            start_time = time.time()\n",
    "\n",
    "            print(\"\\n epoch %d/%d:\" % (ep, total_epoch))  # 1/200形式\n",
    "\n",
    "            for it in range(1, iterations + 1):  # 训练集区域\n",
    "                batch_x = train_x[pre_index:pre_index + batch_size]  # 索引取训练数据\n",
    "                batch_y = train_y[pre_index:pre_index + batch_size]\n",
    "\n",
    "                batch_x = data_augmentation(batch_x) #将取出的训练数据拿去增强\n",
    "\n",
    "                _, batch_loss = sess.run([train_step, cross_entropy],  # 启动训练，batch_loss获得loss值\n",
    "                                         feed_dict={x: batch_x, y_: batch_y, learning_rate: lr})\n",
    "\n",
    "                batch_acc = accuracy.eval(feed_dict={x: batch_x, y_: batch_y})\n",
    "\n",
    "                train_loss += batch_loss  # 累加\n",
    "                train_acc += batch_acc  # 累加\n",
    "                pre_index += batch_size  # 这个累加用于更新取值范围\n",
    "\n",
    "                if it == iterations:\n",
    "                    train_loss /= iterations  # 求平均值，更合理\n",
    "                    train_acc /= iterations\n",
    "\n",
    "                    loss_, acc_ = sess.run([cross_entropy, accuracy],\n",
    "                                           feed_dict={x: batch_x, y_: batch_y})\n",
    "\n",
    "                    summary = sess.run(merged, feed_dict={x: batch_x, y_: batch_y})\n",
    "                    train_writer.add_summary(summary, ep)\n",
    "\n",
    "                    # 为tensorboard结构图用的\n",
    "                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss),\n",
    "                                                      tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
    "\n",
    "                    val_acc, val_loss, test_summary = run_testing(sess, ep)  # 传到测试集函数区域做计算取值\n",
    "\n",
    "                    test_writer.add_summary(test_summary, ep)\n",
    "\n",
    "                    summary_writer.add_summary(train_summary, ep)\n",
    "                    summary_writer.add_summary(test_summary, ep)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                    print(\"iteration: %d/%d, cost_time: %ds, train_loss: %.4f, \"\n",
    "                          \"train_acc: %.4f, test_loss: %.4f, test_acc: %.4f\"\n",
    "                          % (it, iterations, int(time.time() - start_time), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "                else:\n",
    "                    print(\"iteration: %d/%d, train_loss: %.4f, train_acc: %.4f\"\n",
    "                          % (it, iterations, train_loss / it, train_acc / it), end='\\r')  # 一个周期不跑完，不跑训练集结果\n",
    "\n",
    "            tf.summary.merge_all()\n",
    "        save_path = saver.save(sess, model_save_path)  # model储存路径\n",
    "        print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for ep in range(1, total_epoch + 1):\n",
    "            lr = learning_rate_schedule(ep)\n",
    "            pre_index = 0\n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            start_time = time.time()\n",
    "\n",
    "            print(\"\\n epoch %d/%d:\" % (ep, total_epoch))  # 1/200形式\n",
    "\n",
    "            for it in range(1, iterations + 1):  # 训练集区域\n",
    "                batch_x = train_x[pre_index:pre_index + batch_size]  # 索引取训练数据\n",
    "                batch_y = train_y[pre_index:pre_index + batch_size]\n",
    "\n",
    "                batch_x = data_augmentation(batch_x) #将取出的训练数据拿去增强\n",
    "\n",
    "                _, batch_loss = sess.run([train_step, cross_entropy],  # 启动训练，batch_loss获得loss值\n",
    "                                         feed_dict={x: batch_x, y_: batch_y, learning_rate: lr})\n",
    "\n",
    "                batch_acc = accuracy.eval(feed_dict={x: batch_x, y_: batch_y})\n",
    "\n",
    "                train_loss += batch_loss  # 累加\n",
    "                train_acc += batch_acc  # 累加\n",
    "                pre_index += batch_size  # 这个累加用于更新取值范围\n",
    "\n",
    "                if it == iterations:\n",
    "                    train_loss /= iterations  # 求平均值，更合理\n",
    "                    train_acc /= iterations\n",
    "\n",
    "                    loss_, acc_ = sess.run([cross_entropy, accuracy],\n",
    "                                           feed_dict={x: batch_x, y_: batch_y})\n",
    "\n",
    "                    summary = sess.run(merged, feed_dict={x: batch_x, y_: batch_y})\n",
    "                    train_writer.add_summary(summary, ep)\n",
    "\n",
    "                    # 为tensorboard结构图用的\n",
    "                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss),\n",
    "                                                      tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
    "\n",
    "                    val_acc, val_loss, test_summary = run_testing(sess, ep)  # 传到测试集函数区域做计算取值\n",
    "\n",
    "                    test_writer.add_summary(test_summary, ep)\n",
    "\n",
    "                    summary_writer.add_summary(train_summary, ep)\n",
    "                    summary_writer.add_summary(test_summary, ep)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                    print(\"iteration: %d/%d, cost_time: %ds, train_loss: %.4f, \"\n",
    "                          \"train_acc: %.4f, test_loss: %.4f, test_acc: %.4f\"\n",
    "                          % (it, iterations, int(time.time() - start_time), train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "                else:\n",
    "                    print(\"iteration: %d/%d, train_loss: %.4f, train_acc: %.4f\"\n",
    "                          % (it, iterations, train_loss / it, train_acc / it), end='\\r')  # 一个周期不跑完，不跑训练集结果\n",
    "\n",
    "            tf.summary.merge_all()\n",
    "        save_path = saver.save(sess, model_save_path)  # model储存路径\n",
    "        print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_v2_50.fdsf = 333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
